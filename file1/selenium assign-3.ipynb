{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79d3e09d",
   "metadata": {},
   "source": [
    "### 1. Write a python program which searches all the product under a particular product from www.amazon.in. \n",
    "### The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0b40354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product to search on Amazon: laptop\n",
      "                                         Product Name\n",
      "0   Apple 2023 MacBook Pro (16-inch, M3 Max chip w...\n",
      "1   Apple 2023 MacBook Air laptop with M2 chip: 38...\n",
      "2   HP Laptop 15s, 12th Gen Intel Core i3, 15.6-in...\n",
      "3   ASUS VivoBook 15 (2021), 15.6-inch (39.62 cm) ...\n",
      "4   HP Laptop 15s, Intel Celeron, 15.6-inch (39.6 ...\n",
      "5   HP 15s 12th Gen Intel Core i5, 15/6inch (39.6 ...\n",
      "6   ASUS Vivobook 16 (2023), Intel Core i9-13900H ...\n",
      "7   HP Laptop 15s, AMD Ryzen 3 5300U, 15.6-inch (3...\n",
      "8                                                    \n",
      "9                                                    \n",
      "10  ASUS Vivobook 16 (2023), Intel Core i9-13900H ...\n",
      "11  HP Laptop 15s, 12th Gen Intel Core i5-1235U, 1...\n",
      "12  Chuwi HeroBook Pro 14.1'' Intel Celeron N4020 ...\n",
      "13  HP 2023 Ryzen 3 Dual Core 3250U - (8 GB/512 GB...\n",
      "14  ASUS Vivobook 15 (2023), Intel Core i5-1335U 1...\n",
      "15  MSI GF63 Thin, Intel 11th Gen. i5-11260H, 40CM...\n",
      "16  Apple 2023 MacBook Pro (14-inch, M3 Max chip w...\n",
      "17  HP 15s 12th Gen Intel Core i5, 15/6inch (39.6 ...\n",
      "18  Lenovo IdeaPad Slim 3 Intel Core i3 12th Gen 1...\n",
      "19  TECNO MEGABOOK T1,Intel Core 11th Gen i3 Proce...\n",
      "20  HP Laptop 14s, Intel Pentium Silver N6000, 14 ...\n",
      "21  Lenovo ThinkBook 15 Intel 12th Gen Core i7 15....\n",
      "22  HP 15s 12th Gen Intel Core i5, 15/6inch (39.6 ...\n",
      "23  AXL VayuBook Laptop 14.1 Inch FHD IPS Display ...\n",
      "24  Lenovo LOQ Intel Core i5-12450H 15.6\" (39.6cm)...\n",
      "25  Acer Travelmate Business Laptop AMD Ryzen 5 55...\n",
      "26  Dell 14 Laptop, AMD Ryzen 5-5500U/ 8GB- 2 DIMM...\n",
      "27  Dell 14 Laptop, 12th Gen Intel Core i3-1215U P...\n",
      "28  Chuwi HeroBook Pro 14.1'' Intel Celeron N4020 ...\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Function to search for a product on Amazon and store results in a DataFrame\n",
    "def search_amazon_product(product_name):\n",
    "    # Initialize Chrome WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    try:\n",
    "        # Open Amazon website\n",
    "        driver.get(\"https://www.amazon.in/\")\n",
    "        \n",
    "        # Wait for the page to load\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find the search box element using XPath\n",
    "        search_box = driver.find_element(By.XPATH, '//*[@id=\"twotabsearchtextbox\"]')\n",
    "        \n",
    "        # Input the product name into the search box\n",
    "        search_box.send_keys(product_name)\n",
    "        \n",
    "        # Press Enter key to perform the search\n",
    "        search_box.send_keys(Keys.RETURN)\n",
    "        \n",
    "        time.sleep(2)  # Wait for 2 seconds for the page to load\n",
    "        \n",
    "        # Find all the product elements\n",
    "        products = driver.find_elements(By.CSS_SELECTOR, \".s-result-item h2 a\")\n",
    "        \n",
    "        # Create an empty list to store the product names\n",
    "        product_names = []\n",
    "        \n",
    "        # Store product names in the list\n",
    "        for product in products:\n",
    "            product_names.append(product.text)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        product_names = []  # Return an empty list if an error occurs\n",
    "    \n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "        \n",
    "        # Convert the list of product names into a DataFrame\n",
    "        df = pd.DataFrame(product_names, columns=[\"Product Name\"])\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Take user input for the product to search\n",
    "    product_name = input(\"Enter the product to search on Amazon: \")\n",
    "    \n",
    "    # Call function to search for the product on Amazon and get results as DataFrame\n",
    "    product_df = search_amazon_product(product_name)\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(product_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9747ad17",
   "metadata": {},
   "source": [
    "#### 2. In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a1e2ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product to search on Amazon: laptop\n",
      "Searching for the product on Amazon...\n",
      "Scraping details from the first page...\n",
      "Scraping product details...\n",
      "Product details scraped successfully.\n",
      "Scraping details from page 2...\n",
      "Scraping product details...\n",
      "Product details scraped successfully.\n",
      "Scraping details from page 3...\n",
      "Scraping product details...\n",
      "Product details scraped successfully.\n",
      "Scraped details saved to 'amazon_products.csv'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Function to scrape details of each product listed in search results\n",
    "def scrape_product_details(driver):\n",
    "    print(\"Scraping product details...\")\n",
    "    # Find all the product elements\n",
    "    products = driver.find_elements(By.CSS_SELECTOR, \".s-result-item\")\n",
    "    \n",
    "    # Initialize lists to store details\n",
    "    brand_names = []\n",
    "    product_names = []\n",
    "    prices = []\n",
    "    return_exchange = []\n",
    "    expected_delivery = []\n",
    "    availabilities = []\n",
    "    product_urls = []\n",
    "    \n",
    "    # Loop through each product\n",
    "    for product in products:\n",
    "        try:\n",
    "            # Extracting details if available\n",
    "            brand_name = product.find_element(By.CSS_SELECTOR, \".a-size-base-plus.a-color-base\").text.strip()\n",
    "        except:\n",
    "            brand_name = \"-\"\n",
    "        \n",
    "        try:\n",
    "            product_name = product.find_element(By.CSS_SELECTOR, \".s-line-clamp-2\").text.strip()\n",
    "        except:\n",
    "            product_name = \"-\"\n",
    "        \n",
    "        try:\n",
    "            price = product.find_element(By.CSS_SELECTOR, \".a-price .a-offscreen\").text.strip()\n",
    "        except:\n",
    "            price = \"-\"\n",
    "        \n",
    "        try:\n",
    "            ret_exchange = product.find_element(By.CSS_SELECTOR, \".a-icon-alt\").text.strip()\n",
    "        except:\n",
    "            ret_exchange = \"-\"\n",
    "        \n",
    "        try:\n",
    "            delivery = product.find_element(By.CSS_SELECTOR, \".a-text-bold\").text.strip()\n",
    "        except:\n",
    "            delivery = \"-\"\n",
    "        \n",
    "        try:\n",
    "            availability = product.find_element(By.CSS_SELECTOR, \".a-icon-check-square\").text.strip()\n",
    "        except:\n",
    "            availability = \"-\"\n",
    "        \n",
    "        try:\n",
    "            product_url = product.find_element(By.CSS_SELECTOR, \"a.a-link-normal\").get_attribute(\"href\").strip()\n",
    "        except:\n",
    "            product_url = \"-\"\n",
    "        \n",
    "        # Append details to respective lists\n",
    "        brand_names.append(brand_name)\n",
    "        product_names.append(product_name)\n",
    "        prices.append(price)\n",
    "        return_exchange.append(ret_exchange)\n",
    "        expected_delivery.append(delivery)\n",
    "        availabilities.append(availability)\n",
    "        product_urls.append(product_url)\n",
    "    \n",
    "    # Create a DataFrame from the scraped details\n",
    "    df = pd.DataFrame({\n",
    "        \"Brand Name\": brand_names,\n",
    "        \"Name of the Product\": product_names,\n",
    "        \"Price\": prices,\n",
    "        \"Return/Exchange\": return_exchange,\n",
    "        \"Expected Delivery\": expected_delivery,\n",
    "        \"Availability\": availabilities,\n",
    "        \"Product URL\": product_urls\n",
    "    })\n",
    "    print(\"Product details scraped successfully.\")\n",
    "    return df\n",
    "\n",
    "# Function to search for a product on Amazon, scrape details from first 3 pages, and save to CSV\n",
    "def search_amazon_product(product_name):\n",
    "    print(\"Searching for the product on Amazon...\")\n",
    "    # Initialize Chrome WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    try:\n",
    "        # Open Amazon website\n",
    "        driver.get(\"https://www.amazon.in/\")\n",
    "        \n",
    "        # Wait for the page to load\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Find the search box element using XPath\n",
    "        search_box = driver.find_element(By.XPATH, '//*[@id=\"twotabsearchtextbox\"]')\n",
    "        \n",
    "        # Input the product name into the search box\n",
    "        search_box.send_keys(product_name)\n",
    "        \n",
    "        # Press Enter key to perform the search\n",
    "        search_box.send_keys(Keys.RETURN)\n",
    "        \n",
    "        time.sleep(2)  # Wait for 2 seconds for the page to load\n",
    "        \n",
    "        # Scrape details from the first page\n",
    "        print(\"Scraping details from the first page...\")\n",
    "        df = scrape_product_details(driver)\n",
    "        \n",
    "        # Loop through next 2 pages and scrape details\n",
    "        for i in range(2):\n",
    "            # Find the next page button and click\n",
    "            next_page_button = driver.find_element(By.CSS_SELECTOR, \".a-last a\")\n",
    "            next_page_button.click()\n",
    "            time.sleep(2)  # Wait for 2 seconds for the page to load\n",
    "            \n",
    "            # Scrape details from the current page and append to DataFrame\n",
    "            print(f\"Scraping details from page {i+2}...\")\n",
    "            df = pd.concat([df, scrape_product_details(driver)], ignore_index=True)\n",
    "        \n",
    "        # Save DataFrame to CSV\n",
    "        df.to_csv(\"amazon_products.csv\", index=False)\n",
    "        print(\"Scraped details saved to 'amazon_products.csv'\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "    \n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Take user input for the product to search\n",
    "    product_name = input(\"Enter the product to search on Amazon: \")\n",
    "    \n",
    "    # Call function to search for the product on Amazon and scrape details\n",
    "    search_amazon_product(product_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c337bf2",
   "metadata": {},
   "source": [
    "# 3. Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4656474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping images for 'fruits'...\n",
      "An error occurred while scraping images for 'fruits': Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF6605170C2+63090]\n",
      "\t(No symbol) [0x00007FF660482D12]\n",
      "\t(No symbol) [0x00007FF66031EC65]\n",
      "\t(No symbol) [0x00007FF66036499D]\n",
      "\t(No symbol) [0x00007FF660364ADC]\n",
      "\t(No symbol) [0x00007FF6603A5B37]\n",
      "\t(No symbol) [0x00007FF66038701F]\n",
      "\t(No symbol) [0x00007FF6603A3412]\n",
      "\t(No symbol) [0x00007FF660386D83]\n",
      "\t(No symbol) [0x00007FF6603583A8]\n",
      "\t(No symbol) [0x00007FF660359441]\n",
      "\tGetHandleVerifier [0x00007FF66091262D+4238301]\n",
      "\tGetHandleVerifier [0x00007FF66094F78D+4488509]\n",
      "\tGetHandleVerifier [0x00007FF660947A6F+4456479]\n",
      "\tGetHandleVerifier [0x00007FF6605F0606+953270]\n",
      "\t(No symbol) [0x00007FF66048E5DF]\n",
      "\t(No symbol) [0x00007FF6604892B4]\n",
      "\t(No symbol) [0x00007FF6604893EB]\n",
      "\t(No symbol) [0x00007FF660479C24]\n",
      "\tBaseThreadInitThunk [0x00007FFED5D9257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFED778AA58+40]\n",
      "\n",
      "Scraping images for 'cars'...\n",
      "An error occurred while scraping images for 'cars': Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF6605170C2+63090]\n",
      "\t(No symbol) [0x00007FF660482D12]\n",
      "\t(No symbol) [0x00007FF66031EC65]\n",
      "\t(No symbol) [0x00007FF66036499D]\n",
      "\t(No symbol) [0x00007FF660364ADC]\n",
      "\t(No symbol) [0x00007FF6603A5B37]\n",
      "\t(No symbol) [0x00007FF66038701F]\n",
      "\t(No symbol) [0x00007FF6603A3412]\n",
      "\t(No symbol) [0x00007FF660386D83]\n",
      "\t(No symbol) [0x00007FF6603583A8]\n",
      "\t(No symbol) [0x00007FF660359441]\n",
      "\tGetHandleVerifier [0x00007FF66091262D+4238301]\n",
      "\tGetHandleVerifier [0x00007FF66094F78D+4488509]\n",
      "\tGetHandleVerifier [0x00007FF660947A6F+4456479]\n",
      "\tGetHandleVerifier [0x00007FF6605F0606+953270]\n",
      "\t(No symbol) [0x00007FF66048E5DF]\n",
      "\t(No symbol) [0x00007FF6604892B4]\n",
      "\t(No symbol) [0x00007FF6604893EB]\n",
      "\t(No symbol) [0x00007FF660479C24]\n",
      "\tBaseThreadInitThunk [0x00007FFED5D9257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFED778AA58+40]\n",
      "\n",
      "Scraping images for 'Machine Learning'...\n",
      "An error occurred while scraping images for 'Machine Learning': Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF6605170C2+63090]\n",
      "\t(No symbol) [0x00007FF660482D12]\n",
      "\t(No symbol) [0x00007FF66031EC65]\n",
      "\t(No symbol) [0x00007FF66036499D]\n",
      "\t(No symbol) [0x00007FF660364ADC]\n",
      "\t(No symbol) [0x00007FF6603A5B37]\n",
      "\t(No symbol) [0x00007FF66038701F]\n",
      "\t(No symbol) [0x00007FF6603A3412]\n",
      "\t(No symbol) [0x00007FF660386D83]\n",
      "\t(No symbol) [0x00007FF6603583A8]\n",
      "\t(No symbol) [0x00007FF660359441]\n",
      "\tGetHandleVerifier [0x00007FF66091262D+4238301]\n",
      "\tGetHandleVerifier [0x00007FF66094F78D+4488509]\n",
      "\tGetHandleVerifier [0x00007FF660947A6F+4456479]\n",
      "\tGetHandleVerifier [0x00007FF6605F0606+953270]\n",
      "\t(No symbol) [0x00007FF66048E5DF]\n",
      "\t(No symbol) [0x00007FF6604892B4]\n",
      "\t(No symbol) [0x00007FF6604893EB]\n",
      "\t(No symbol) [0x00007FF660479C24]\n",
      "\tBaseThreadInitThunk [0x00007FFED5D9257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFED778AA58+40]\n",
      "\n",
      "Scraping images for 'Guitar'...\n",
      "An error occurred while scraping images for 'Guitar': Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF6605170C2+63090]\n",
      "\t(No symbol) [0x00007FF660482D12]\n",
      "\t(No symbol) [0x00007FF66031EC65]\n",
      "\t(No symbol) [0x00007FF66036499D]\n",
      "\t(No symbol) [0x00007FF660364ADC]\n",
      "\t(No symbol) [0x00007FF6603A5B37]\n",
      "\t(No symbol) [0x00007FF66038701F]\n",
      "\t(No symbol) [0x00007FF6603A3412]\n",
      "\t(No symbol) [0x00007FF660386D83]\n",
      "\t(No symbol) [0x00007FF6603583A8]\n",
      "\t(No symbol) [0x00007FF660359441]\n",
      "\tGetHandleVerifier [0x00007FF66091262D+4238301]\n",
      "\tGetHandleVerifier [0x00007FF66094F78D+4488509]\n",
      "\tGetHandleVerifier [0x00007FF660947A6F+4456479]\n",
      "\tGetHandleVerifier [0x00007FF6605F0606+953270]\n",
      "\t(No symbol) [0x00007FF66048E5DF]\n",
      "\t(No symbol) [0x00007FF6604892B4]\n",
      "\t(No symbol) [0x00007FF6604893EB]\n",
      "\t(No symbol) [0x00007FF660479C24]\n",
      "\tBaseThreadInitThunk [0x00007FFED5D9257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFED778AA58+40]\n",
      "\n",
      "Scraping images for 'Cakes'...\n",
      "An error occurred while scraping images for 'Cakes': Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF6605170C2+63090]\n",
      "\t(No symbol) [0x00007FF660482D12]\n",
      "\t(No symbol) [0x00007FF66031EC65]\n",
      "\t(No symbol) [0x00007FF66036499D]\n",
      "\t(No symbol) [0x00007FF660364ADC]\n",
      "\t(No symbol) [0x00007FF6603A5B37]\n",
      "\t(No symbol) [0x00007FF66038701F]\n",
      "\t(No symbol) [0x00007FF6603A3412]\n",
      "\t(No symbol) [0x00007FF660386D83]\n",
      "\t(No symbol) [0x00007FF6603583A8]\n",
      "\t(No symbol) [0x00007FF660359441]\n",
      "\tGetHandleVerifier [0x00007FF66091262D+4238301]\n",
      "\tGetHandleVerifier [0x00007FF66094F78D+4488509]\n",
      "\tGetHandleVerifier [0x00007FF660947A6F+4456479]\n",
      "\tGetHandleVerifier [0x00007FF6605F0606+953270]\n",
      "\t(No symbol) [0x00007FF66048E5DF]\n",
      "\t(No symbol) [0x00007FF6604892B4]\n",
      "\t(No symbol) [0x00007FF6604893EB]\n",
      "\t(No symbol) [0x00007FF660479C24]\n",
      "\tBaseThreadInitThunk [0x00007FFED5D9257D+29]\n",
      "\tRtlUserThreadStart [0x00007FFED778AA58+40]\n",
      "\n",
      "\n",
      "fruits:\n",
      "\n",
      "\n",
      "cars:\n",
      "\n",
      "\n",
      "Machine Learning:\n",
      "\n",
      "\n",
      "Guitar:\n",
      "\n",
      "\n",
      "Cakes:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# Function to scrape image URLs from Google Images search results\n",
    "def scrape_image_urls(driver, keyword, num_images):\n",
    "    try:\n",
    "        # Wait for the search bar to be clickable\n",
    "        search_bar = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//input[@class=\"gLFyf gsfi\"]')))\n",
    "        \n",
    "        # Clear the search bar and enter the keyword\n",
    "        search_bar.clear()\n",
    "        search_bar.send_keys(keyword)\n",
    "        \n",
    "        # Press Enter to perform the search\n",
    "        search_bar.send_keys(Keys.RETURN)\n",
    "        \n",
    "        time.sleep(2)  # Wait for 2 seconds for the search results to load\n",
    "        \n",
    "        # Scroll down to load more images\n",
    "        for _ in range(3):\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "        \n",
    "        # Find all the image elements\n",
    "        image_elements = driver.find_elements(By.XPATH, '//img[@class=\"rg_i Q4LuWd\"]')\n",
    "        \n",
    "        # Extract image URLs\n",
    "        image_urls = []\n",
    "        for i in range(min(num_images, len(image_elements))):\n",
    "            try:\n",
    "                # Click on the image to load the full-size image\n",
    "                image_elements[i].click()\n",
    "                time.sleep(2)  # Wait for the full-size image to load\n",
    "                \n",
    "                # Find the full-size image URL\n",
    "                full_size_image = driver.find_element(By.XPATH, '//img[@class=\"n3VNCb\"]').get_attribute('src')\n",
    "                \n",
    "                if full_size_image:\n",
    "                    image_urls.append(full_size_image)\n",
    "            except Exception as e:\n",
    "                print(f\"Error while scraping image {i+1} for '{keyword}':\", e)\n",
    "        \n",
    "        return image_urls\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while scraping images for '{keyword}':\", e)\n",
    "        return []\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Initialize Chrome WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    try:\n",
    "        # Open Google Images website\n",
    "        driver.get(\"https://www.google.com/imghp\")\n",
    "        \n",
    "        time.sleep(2)  # Wait for 2 seconds for the page to load\n",
    "        \n",
    "        # Keywords to search for images\n",
    "        keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "        \n",
    "        # Number of images to scrape for each keyword\n",
    "        num_images_per_keyword = 10\n",
    "        \n",
    "        # Dictionary to store image URLs for each keyword\n",
    "        image_urls_dict = {}\n",
    "        \n",
    "        # Scrape images for each keyword\n",
    "        for keyword in keywords:\n",
    "            print(f\"Scraping images for '{keyword}'...\")\n",
    "            image_urls = scrape_image_urls(driver, keyword, num_images_per_keyword)\n",
    "            image_urls_dict[keyword] = image_urls\n",
    "        \n",
    "        # Print the scraped image URLs\n",
    "        for keyword, urls in image_urls_dict.items():\n",
    "            print(f\"\\n{keyword}:\\n\")\n",
    "            for i, url in enumerate(urls, start=1):\n",
    "                print(f\"{i}. {url}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "    \n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d9d469",
   "metadata": {},
   "source": [
    "# 4] Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ba96259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter smartphone name to search on Flipkart: Oneplus\n",
      "Data saved to smartphones.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_flipkart_smartphones(search_query):\n",
    "    url = f\"https://www.flipkart.com/search?q={search_query}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    smartphones = []\n",
    "    results = soup.find_all(\"div\", class_=\"_1AtVbE\")\n",
    "\n",
    "    for result in results:\n",
    "        details = {}\n",
    "\n",
    "        brand_element = result.find(\"div\", class_=\"_4rR01T\")\n",
    "        details[\"Brand Name\"] = brand_element.text.strip() if brand_element else \"-\"\n",
    "\n",
    "        name_element = result.find(\"a\", class_=\"IRpwTa\")\n",
    "        details[\"Smartphone Name\"] = name_element.text.strip() if name_element else \"-\"\n",
    "        details[\"Colour\"] = name_element.get(\"title\").split(\"(\")[-1].strip(\")\") if name_element else \"-\"\n",
    "\n",
    "        details[\"RAM\"] = result.find(string=\"RAM\").find_next(\"li\").text.strip() if result.find(string=\"RAM\") else \"-\"\n",
    "        details[\"Storage(ROM)\"] = result.find(string=\"ROM\").find_next(\"li\").text.strip() if result.find(string=\"ROM\") else \"-\"\n",
    "        details[\"Primary Camera\"] = result.find(string=\"Primary Camera\").find_next(\"li\").text.strip() if result.find(string=\"Primary Camera\") else \"-\"\n",
    "        details[\"Secondary Camera\"] = result.find(string=\"Secondary Camera\").find_next(\"li\").text.strip() if result.find(string=\"Secondary Camera\") else \"-\"\n",
    "        details[\"Display Size\"] = result.find(string=\"Display Size\").find_next(\"li\").text.strip() if result.find(string=\"Display Size\") else \"-\"\n",
    "        details[\"Battery Capacity\"] = result.find(string=\"Battery Capacity\").find_next(\"li\").text.strip() if result.find(string=\"Battery Capacity\") else \"-\"\n",
    "        details[\"Price\"] = result.find(\"div\", class_=\"_30jeq3 _1_WHN1\").text.strip() if result.find(\"div\", class_=\"_30jeq3 _1_WHN1\") else \"-\"\n",
    "        details[\"Product URL\"] = \"https://www.flipkart.com\" + name_element.get(\"href\") if name_element else \"-\"\n",
    "\n",
    "        smartphones.append(details)\n",
    "\n",
    "    return smartphones\n",
    "\n",
    "def save_to_csv(data, filename=\"smartphones.csv\"):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    search_query = input(\"Enter smartphone name to search on Flipkart: \")\n",
    "    smartphones = scrape_flipkart_smartphones(search_query)\n",
    "    if smartphones:\n",
    "        save_to_csv(smartphones)\n",
    "    else:\n",
    "        print(\"No results found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6dcd0a",
   "metadata": {},
   "source": [
    "### 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3181dfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the city name: Pune\n",
      "Coordinates for Pune: Latitude 18.521428, Longitude 73.8544541\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_coordinates(city):\n",
    "    # Nominatim geocoding API endpoint\n",
    "    url = \"https://nominatim.openstreetmap.org/search\"\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"q\": city,\n",
    "        \"format\": \"json\",\n",
    "        \"limit\": 1\n",
    "    }\n",
    "\n",
    "    # Send a GET request to the Nominatim API\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "\n",
    "    # Parse the response to extract coordinates\n",
    "    if data:\n",
    "        latitude = data[0][\"lat\"]\n",
    "        longitude = data[0][\"lon\"]\n",
    "        print(f\"Coordinates for {city}: Latitude {latitude}, Longitude {longitude}\")\n",
    "    else:\n",
    "        print(f\"Coordinates for {city} not found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city = input(\"Enter the city name: \")\n",
    "    get_coordinates(city)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8706441f",
   "metadata": {},
   "source": [
    "## 6. Write a program to scrap all the available details of best gaming laptops from digit.in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eccabf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_digit_in():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        laptops = soup.find_all('div', class_='TopNumbeHeading')\n",
    "        \n",
    "        for laptop in laptops:\n",
    "            name = laptop.find('div', class_='TopNumbeHeading').text.strip()\n",
    "            specs = laptop.find('div', class_='Specs').text.strip()\n",
    "            price = laptop.find('div', class_='smprice').text.strip()\n",
    "            \n",
    "            print(f\"Name: {name}\")\n",
    "            print(f\"Specifications: {specs}\")\n",
    "            print(f\"Price: {price}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from the website.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_digit_in()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305554f8",
   "metadata": {},
   "source": [
    "## 7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83cf8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "    \n",
    "    # Start a webdriver session\n",
    "    driver = webdriver.Chrome()  # You may need to adjust this based on your WebDriver configuration\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait for the page to load\n",
    "    driver.implicitly_wait(10)\n",
    "    \n",
    "    # Get the page source\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    # Find all billionaire elements\n",
    "    billionaires = soup.find_all('div', class_='personList')\n",
    "    \n",
    "    for person in billionaires:\n",
    "        rank = person.find('div', class_='rank').text.strip()\n",
    "        name = person.find('div', class_='personName').text.strip()\n",
    "        net_worth = person.find('div', class_='netWorth').text.strip()\n",
    "        age = person.find('div', class_='age').text.strip()\n",
    "        citizenship = person.find('div', class_='countryOfCitizenship').text.strip()\n",
    "        source = person.find('div', class_='source-column').text.strip()\n",
    "        industry = person.find('div', class_='category').text.strip()\n",
    "        \n",
    "        print(\"Rank:\", rank)\n",
    "        print(\"Name:\", name)\n",
    "        print(\"Net Worth:\", net_worth)\n",
    "        print(\"Age:\", age)\n",
    "        print(\"Citizenship:\", citizenship)\n",
    "        print(\"Source:\", source)\n",
    "        print(\"Industry:\", industry)\n",
    "        print()\n",
    "    \n",
    "    # Close the webdriver session\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_forbes_billionaires()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667d5d56",
   "metadata": {},
   "source": [
    "# 9.  Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccce406a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data from Hostelworld.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_hostels_in_london():\n",
    "    url = \"https://www.hostelworld.com/\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        hostels = soup.find_all('div', class_='description-container')\n",
    "        \n",
    "        for hostel in hostels:\n",
    "            name = hostel.find('h2', class_='title').text.strip()\n",
    "            distance = hostel.find('span', class_='description').text.strip()\n",
    "            rating = hostel.find('div', class_='score').text.strip()\n",
    "            total_reviews = hostel.find('div', class_='reviews').text.strip().split()[0]\n",
    "            overall_reviews = hostel.find('div', class_='ratingtext').text.strip()\n",
    "            privates_from_price = hostel.find('span', class_='price').text.strip().split('\\n')[0].strip()\n",
    "            dorms_from_price = hostel.find('span', class_='price').text.strip().split('\\n')[1].strip()\n",
    "            facilities = [facility.text.strip() for facility in hostel.find_all('span', class_='label')]\n",
    "            description = hostel.find('div', class_='additional-info').text.strip()\n",
    "            \n",
    "            print(\"Name:\", name)\n",
    "            print(\"Distance from city centre:\", distance)\n",
    "            print(\"Rating:\", rating)\n",
    "            print(\"Total reviews:\", total_reviews)\n",
    "            print(\"Overall reviews:\", overall_reviews)\n",
    "            print(\"Privates from price:\", privates_from_price)\n",
    "            print(\"Dorms from price:\", dorms_from_price)\n",
    "            print(\"Facilities:\", facilities)\n",
    "            print(\"Description:\", description)\n",
    "            print()\n",
    "    else:\n",
    "        print(\"Failed to retrieve data from Hostelworld.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_hostels_in_london()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165e39e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
